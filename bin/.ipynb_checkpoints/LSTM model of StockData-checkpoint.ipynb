{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Stock Data\n",
    "\n",
    "This notebook covers a  basic Long SHort Term Memory (LSTM)model for time series. \n",
    "\n",
    "In This Notebook\n",
    " - First load the data. The preproccessing only conssist of normallization and thecreation of windows.\n",
    " - Creating LSTM Model\n",
    " - Training LSTM Model\n",
    " - Testing LSTM model with 1 timestamp and with 1 window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and loading the data\n",
    "\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa.seasonal as smt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import datetime as dt\n",
    "from sklearn import linear_model \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import plotly\n",
    "\n",
    "# import the relevant Keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "import os\n",
    "os.chdir('../input/Data/Stocks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "# kernels let us navigate through the zipfile as if it were a directory\n",
    "\n",
    "# trying to read a file of size zero will throw an error, so skip them\n",
    "# filenames = [x for x in os.listdir() if x.endswith('.txt') and os.path.getsize(x) > 0]\n",
    "# filenames = random.sample(filenames,1)\n",
    "filenames = ['prk.us.txt', 'bgr.us.txt', 'jci.us.txt', 'aa.us.txt', 'fr.us.txt', 'star.us.txt', 'sons.us.txt', 'ipl_d.us.txt', 'sna.us.txt', 'utg.us.txt']\n",
    "filenames = [filenames[1]]\n",
    "print(filenames)\n",
    "\n",
    "data = []\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(filename, sep=',')\n",
    "\n",
    "    label, _, _ = filename.split(sep='.')\n",
    "    df['Label'] = filename\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = lambda: random.randint(0,255)\n",
    "traces = []\n",
    "\n",
    "for df in data:\n",
    "    clr = str(r()) + str(r()) + str(r())\n",
    "#     df = df.sample(n=100, replace=True)\n",
    "    df = df.sort_values('Date')\n",
    "#     print(df['Label'])\n",
    "    label = df['Label'].iloc[0]\n",
    "\n",
    "    trace = plotly.graph_objs.Scattergl(\n",
    "        x=df['Date'],\n",
    "        y=df['Close'],\n",
    "        mode='line',\n",
    "        line=dict(\n",
    "            color = clr\n",
    "        )\n",
    "    )\n",
    "    traces.append(trace)\n",
    "    \n",
    "layout = plotly.graph_objs.Layout(\n",
    "    title='Plot',\n",
    ")\n",
    "fig = plotly.graph_objs.Figure(data=traces, layout=layout)\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "plotly.offline.iplot(fig, filename='dataplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating windows and normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[0]\n",
    "window_len = 10\n",
    "\n",
    "#Create a data point (i.e. a date) which splits the training and testing set\n",
    "split_date = list(data[0][\"Date\"][-(2*window_len+1):])[0]\n",
    "\n",
    "#Split the training and test set\n",
    "training_set, test_set = df[df['Date'] < split_date], df[df['Date'] >= split_date]\n",
    "training_set = training_set.drop(['Date','Label', 'OpenInt'], 1)\n",
    "test_set = test_set.drop(['Date','Label','OpenInt'], 1)\n",
    "\n",
    "#Create windows for training\n",
    "LSTM_training_inputs = []\n",
    "for i in range(len(training_set)-window_len):\n",
    "    temp_set = training_set[i:(i+window_len)].copy()\n",
    "    \n",
    "    for col in list(temp_set):\n",
    "        temp_set[col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
    "    \n",
    "    LSTM_training_inputs.append(temp_set)\n",
    "LSTM_training_outputs = (training_set['Close'][window_len:].values/training_set['Close'][:-window_len].values)-1\n",
    "\n",
    "LSTM_training_inputs = [np.array(LSTM_training_input) for LSTM_training_input in LSTM_training_inputs]\n",
    "LSTM_training_inputs = np.array(LSTM_training_inputs)\n",
    "\n",
    "#Create windows for testing\n",
    "LSTM_test_inputs = []\n",
    "for i in range(len(test_set)-window_len):\n",
    "    temp_set = test_set[i:(i+window_len)].copy()\n",
    "    \n",
    "    for col in list(temp_set):\n",
    "        temp_set[col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
    "    \n",
    "    LSTM_test_inputs.append(temp_set)\n",
    "LSTM_test_outputs = (test_set['Close'][window_len:].values/test_set['Close'][:-window_len].values)-1\n",
    "\n",
    "LSTM_test_inputs = [np.array(LSTM_test_inputs) for LSTM_test_inputs in LSTM_test_inputs]\n",
    "LSTM_test_inputs = np.array(LSTM_test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputs, output_size, neurons, activ_func=\"linear\",\n",
    "                dropout=0.10, loss=\"mae\", optimizer=\"adam\"):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model architecture\n",
    "nn_model = build_model(LSTM_training_inputs, output_size=1, neurons = 32)\n",
    "# model output is next price normalised to 10th previous closing price\n",
    "# train model on data\n",
    "# note: eth_history contains information on the training error per epoch\n",
    "nn_history = nn_model.fit(LSTM_training_inputs, LSTM_training_outputs, \n",
    "                            epochs=5, batch_size=1, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of prediction of one data point ahead\n",
    "As can be seen in the plot, one step prediction is not bad. The scale is a bit of, because the data is normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LSTM_test_outputs, label = \"actual\")\n",
    "plt.plot(nn_model.predict(LSTM_test_inputs), label = \"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "MAE = mean_absolute_error(LSTM_test_outputs, nn_model.predict(LSTM_test_inputs))\n",
    "print('The Mean Absolute Error is: {}'.format(MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction of one window (10 time steps) ahead\n",
    "As can be seen in the plot below, the performance quickly degrades when predicting multiple time points ahead. However compered to something like linear regression the performance is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/llSourcell/How-to-Predict-Stock-Prices-Easily-Demo/blob/master/lstm.py\n",
    "def predict_sequence_full(model, data, window_size):\n",
    "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "    curr_frame = data[0]\n",
    "    predicted = []\n",
    "    for i in range(len(data)):\n",
    "        predicted.append(model.predict(curr_frame[np.newaxis,:,:])[0,0])\n",
    "        curr_frame = curr_frame[1:]\n",
    "        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
    "    return predicted\n",
    "\n",
    "predictions = predict_sequence_full(nn_model, LSTM_test_inputs, 10)\n",
    "\n",
    "plt.plot(LSTM_test_outputs, label=\"actual\")\n",
    "plt.plot(predictions, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "MAE = mean_absolute_error(LSTM_test_outputs, predictions)\n",
    "print('The Mean Absolute Error is: {}'.format(MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "LSTM's do not solve time series prediction. The prediction on one time step is not much better then the lag model. If we increase the number of time steps predicted the performance does not degrade as fast as other, more traditional, methods. However in this case we have an increase of around a factor of 4.5 in error. This grows super-linear with the number of time steps we try to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyProjectVirEnv",
   "language": "python",
   "name": "myprojectvirenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
